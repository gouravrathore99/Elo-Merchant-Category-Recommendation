{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7HQTrIwodf9",
   "metadata": {
    "id": "d7HQTrIwodf9"
   },
   "source": [
    "# Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd6fbe",
   "metadata": {
    "id": "77fd6fbe"
   },
   "source": [
    "This notebook contain whole pipeline from data processing, featurization, model building to predicting the target values for Elo Merchant Category Recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315223bd",
   "metadata": {
    "id": "315223bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from joblib import dump, load\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede63162",
   "metadata": {
    "id": "ede63162"
   },
   "source": [
    "### Function to reduce the memory usage by any pandas dataframe variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ecb382",
   "metadata": {
    "id": "56ecb382"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/champs-scalar-coupling/discussion/96655\n",
    "def reduce_mem_usage(df, verbose = False):\n",
    "    \n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 *\n",
    "                                                                                      (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386f0b6",
   "metadata": {
    "id": "9386f0b6"
   },
   "source": [
    "### Function for one hot encoding categorical columns of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c35a84d",
   "metadata": {
    "id": "2c35a84d"
   },
   "outputs": [],
   "source": [
    "def onehotencoder(df, columns):\n",
    "    \"\"\"This function performs one hot encoding on categorical columns in a dataset and concat\n",
    "    those encoded columns to the dataset and drops the original categorical columns. It takes\n",
    "    dataset as dataframe object and categorical column names as list for input.\"\"\"\n",
    "    \n",
    "    for col in columns:\n",
    "        dummy = pd.get_dummies(df[col], prefix = col)\n",
    "        df = pd.concat([df, dummy], axis = 1)\n",
    "        df.drop(col, axis = 1, inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7397bb",
   "metadata": {
    "id": "8d7397bb"
   },
   "source": [
    "## Loading the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kFEXJx42piqT",
   "metadata": {
    "id": "kFEXJx42piqT"
   },
   "source": [
    "### Function to load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e883230f",
   "metadata": {
    "id": "e883230f"
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"This function loads the dataset into dataframes and returns the train, test,\n",
    "    historical_transactions and new_transactions.\"\"\"\n",
    "    \n",
    "    print(\"Loading Data.........\")\n",
    "    train = pd.read_csv(\"data/train.csv\", parse_dates = ['first_active_month'])\n",
    "    train = reduce_mem_usage(train)\n",
    "    test = pd.read_csv(\"data/test.csv\", parse_dates = ['first_active_month'])\n",
    "    test = reduce_mem_usage(test)\n",
    "    historical_transactions = pd.read_csv(\"data/historical_transactions.csv\", parse_dates = ['purchase_date'],\n",
    "                                      dtype = {\"card_id\" : \"category\"})\n",
    "    historical_transactions = reduce_mem_usage(historical_transactions)\n",
    "    new_transactions = pd.read_csv(\"data/new_merchant_transactions.csv\", parse_dates = ['purchase_date'],\n",
    "                                      dtype = {\"card_id\" : \"category\"})\n",
    "    new_transactions = reduce_mem_usage(new_transactions)\n",
    "    print(\"Loading of Data Completed.........\")\n",
    "    \n",
    "    return train, test, historical_transactions, new_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67b4851",
   "metadata": {
    "id": "c67b4851"
   },
   "source": [
    "## Processing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3151ade",
   "metadata": {
    "id": "c3151ade"
   },
   "source": [
    "### Function to process train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca132478",
   "metadata": {
    "id": "ca132478"
   },
   "source": [
    "We find all the test observations that have similar feature_1, feature_2 and feature_3 values as the missing first_active_month observation and impute it with mode of the first_active_month of similar observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eyxh24qW6Ap",
   "metadata": {
    "id": "1eyxh24qW6Ap"
   },
   "outputs": [],
   "source": [
    "def process_train_test(train, test):\n",
    "    \"\"\"This function performs dataprocessing on train and test set and returns processed train and test set.\n",
    "    It takes train and test set as input.\"\"\"\n",
    "    \n",
    "    print(\"Processing train and test set.........\")\n",
    "    test_null = test[test['first_active_month'].isnull()]\n",
    "    test_similar = test[(test.feature_1 == test_null.feature_1.values[0]) & (test.feature_2 == test_null.feature_2.values[0])\n",
    "                    & (test.feature_3 == test_null.feature_3.values[0])]\n",
    "    test.first_active_month[test['first_active_month'].isnull()] = test_similar['first_active_month'].mode()[0]\n",
    "    del test_null\n",
    "    del test_similar\n",
    "    train.to_csv(\"data/train_processed.csv\", index = False)\n",
    "    test.to_csv(\"data/test_processed.csv\", index = False)\n",
    "    print(\"Processing of train and test set Completed.........\")\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otkdlfJIZZAQ",
   "metadata": {
    "id": "otkdlfJIZZAQ"
   },
   "source": [
    "### Function to process historical and new transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c8f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_merchant_id(df):\n",
    "    \"\"\"This function imputes the null merchant ids in historical and new transactions. It takes takes\n",
    "    transaction dataset as dataframe and returns the transaction dataframe after imputing.\"\"\"\n",
    "    \n",
    "    Merchants_Categorical_Columns = [\"merchant_category_id\", \"subsector_id\", \"city_id\", \"state_id\"]\n",
    "    Merchants_Categorical_Dtypes = {col: \"category\" for col in Merchants_Categorical_Columns}\n",
    "    merchants = pd.read_csv(\"data/merchants.csv\", dtype = Merchants_Categorical_Dtypes)\n",
    "\n",
    "    df_null = df[df['merchant_id'].isnull()]\n",
    "    df_null_index = df_null.index\n",
    "    for idx in tqdm(df_null_index):\n",
    "        df_similar = merchants[(merchants.merchant_category_id == df_null.merchant_category_id.loc[idx]) &\n",
    "                               (merchants.subsector_id == df_null.subsector_id.loc[idx]) &\n",
    "                               (merchants.city_id == df_null.city_id.loc[idx])]\n",
    "        if df_similar.shape[0] != 0:\n",
    "            df.merchant_id.loc[idx] = df_similar['merchant_id'].mode()[0]\n",
    "        del df_similar\n",
    "    del df_null\n",
    "    del merchants\n",
    "    df['merchant_id'].fillna('NAN', inplace = True)\n",
    "    df['merchant_id'] = df['merchant_id'].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdcbbde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_category(df, null_columns, train_columns, model_prefix):\n",
    "    \"\"\"This function imputes the null category columns of historical and new transactions\n",
    "    by training classifier model from non null columns. It takes transaction as dataframe,\n",
    "    categorical columns with null values as list, non null columns as list and prefix for\n",
    "    the saved model as string for input.\"\"\"\n",
    "\n",
    "    for col in null_columns:\n",
    "        test_df = df.loc[df[col].isna()][train_columns]\n",
    "        train_df = df.loc[df[col].notna()][train_columns]\n",
    "        train_y = df.loc[df[col].notna()][col]\n",
    "        \n",
    "        path = 'data/' + model_prefix + '_' + str(col) + '_model'\n",
    "        if os.path.exists(path):\n",
    "            clf = pickle.load(open(path, 'rb'))\n",
    "        else:\n",
    "            print(\"Training model to impute\", col)\n",
    "            clf = LogisticRegression()\n",
    "            clf.fit(train_df, train_y)\n",
    "            pickle.dump(clf, open(path, 'wb'))\n",
    "        print(\"Imputing predicted category from model to null values in\", col)\n",
    "        df.loc[df[col].isna(), col] = clf.predict(test_df)\n",
    "        df[col] = df[col].astype(np.int8)\n",
    "        del train_df\n",
    "        del test_df\n",
    "        del train_y\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf45ea",
   "metadata": {},
   "source": [
    "Firstly, the authorized_flag, category_1 and category_3 columns of historical transactions and new transactions are Label Encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d4213",
   "metadata": {
    "id": "7e2d4213"
   },
   "source": [
    "We are finding merchant_ids in merchants having similar merchant_category_id, subsector_id and city_id as null observations and impute null observations with the mode of similar merchant_ids. We are not considering state_id as any two similar city_ids will have same state_id. For remaining merchant_id null values we are simply imputing NAN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719776e9",
   "metadata": {
    "id": "719776e9"
   },
   "source": [
    "We are imputing null values in category_2 and category_3 columns of historical and new transactions by training classifier models from non null columns of these transactions. We will use these classifier models to predict the null values in category_2 and category_3 of historical and new transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801c1da",
   "metadata": {
    "id": "6801c1da"
   },
   "source": [
    "It should be natural to expect the values of purchase amount to be positive which is obviously not the case here. We are using insights provided by Raddar in his notebook in Kaggle for de-anonymizing the purchase amount and transforming the purchase amount into it's observed value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3bf2ab",
   "metadata": {
    "id": "ac3bf2ab"
   },
   "source": [
    "Finally, we are one hot encoding the categorical columns in historical transactions and new transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XRxnsf5QXYdF",
   "metadata": {
    "id": "XRxnsf5QXYdF"
   },
   "outputs": [],
   "source": [
    "def process_transactions(historical_transactions, new_transactions):\n",
    "    \"\"\"This function performs dataprocessing on historical and new transactions and returns a\n",
    "    combined processed transactions dataframe. It takes historical transactions and new transactions\n",
    "    as input.\"\"\"\n",
    "    \n",
    "    print(\"Processing historical and new transactions.........\")\n",
    "    historical_transactions['authorized_flag'] = historical_transactions['authorized_flag'].map({'Y':1,\n",
    "                                                                                                 'N':0}).astype(np.int8)\n",
    "    historical_transactions['category_1'] = historical_transactions['category_1'].map({'Y':1, 'N':0}).astype(np.int8)\n",
    "    historical_transactions['category_3'] = historical_transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "\n",
    "    new_transactions['authorized_flag'] = new_transactions['authorized_flag'].map({'Y':1, 'N':0}).astype(np.int8)\n",
    "    new_transactions['category_1'] = new_transactions['category_1'].map({'Y':1, 'N':0}).astype(np.int8)\n",
    "    new_transactions['category_3'] = new_transactions['category_3'].map({'A':0, 'B':1, 'C':2})\n",
    "    \n",
    "    historical_transactions = impute_merchant_id(historical_transactions)\n",
    "    new_transactions = impute_merchant_id(new_transactions)\n",
    "    \n",
    "    null_columns = ['category_2', 'category_3']\n",
    "    train_columns = ['authorized_flag', 'category_1', 'installments', 'month_lag', 'purchase_amount',\n",
    "                     'merchant_category_id', 'subsector_id', 'city_id', 'state_id']\n",
    "    historical_transactions = impute_category(historical_transactions, null_columns, train_columns, 'historical')\n",
    "    new_transactions = impute_category(new_transactions, null_columns, train_columns, 'new')\n",
    "    \n",
    "    #https://www.kaggle.com/code/raddar/towards-de-anonymizing-the-data-some-insights/notebook\n",
    "    historical_transactions['purchase_amount'] = ((historical_transactions['purchase_amount'].astype(np.float64) /\n",
    "                                                   0.00150265118) + 497.06)\n",
    "    new_transactions['purchase_amount'] = ((new_transactions['purchase_amount'].astype(np.float64) / 0.00150265118) + 497.06)\n",
    "    \n",
    "    categorical_columns = ['category_1', 'category_2', 'category_3']\n",
    "    historical_transactions = onehotencoder(historical_transactions, categorical_columns)\n",
    "    new_transactions = onehotencoder(new_transactions, categorical_columns)\n",
    "    \n",
    "    historical_transactions.to_csv(\"data/historical_transactions_processed.csv\", index = False)\n",
    "    new_transactions.to_csv(\"data/new_transactions_processed.csv\", index = False)\n",
    "    print(\"Processing of historical and new transactions Completed.........\")\n",
    "\n",
    "    return historical_transactions, new_transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d7800",
   "metadata": {
    "id": "f34d7800"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8e32dd",
   "metadata": {
    "id": "ad8e32dd"
   },
   "source": [
    "### Function to perform featurization on train and test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bbae6",
   "metadata": {
    "id": "0b5bbae6"
   },
   "source": [
    "We are using the reference date of 1/2/2018 to calculate the elapsed time for each card id. The elapsed time feature will indicate the number of days, the cardholder has been using the card. We will also divide first active month column into first active year and first active month categorical columns. First active year will denote the year and first active month will denote the month, the cardholder started using the card."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09e415",
   "metadata": {
    "id": "3e09e415"
   },
   "source": [
    "We will be adding outlier identification column to train set. The outlier columns will be 1 for card_ids having outlier value target and 0 for remaining card_ids. This outlier column will be used for stratified splitting of train set during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "RJWTQHf_biPc",
   "metadata": {
    "id": "RJWTQHf_biPc"
   },
   "outputs": [],
   "source": [
    "def feature_train_test(train, test):\n",
    "    \"\"\"This function performs featurization on train and test set. It takes train and test set as input\n",
    "    and returns featurized train and test set.\"\"\"\n",
    "    \n",
    "    print(\"Performing featurization on train and test set.........\")\n",
    "    train['elapsed_time'] = (datetime.date(2018, 2, 1) - train['first_active_month'].dt.date).dt.days\n",
    "    train['first_active_year'] = train['first_active_month'].dt.year\n",
    "    train['first_active_month'] = train['first_active_month'].dt.month\n",
    "    \n",
    "    test['elapsed_time'] = (datetime.date(2018, 2, 1) - test['first_active_month'].dt.date).dt.days\n",
    "    test['first_active_year'] = test['first_active_month'].dt.year\n",
    "    test['first_active_month'] = test['first_active_month'].dt.month\n",
    "\n",
    "    train['outlier'] = 0\n",
    "    train['outlier'][train['target'] > 30] = 1\n",
    "    \n",
    "    train.to_csv(\"data/train_featurized.csv\", index = False)\n",
    "    test.to_csv(\"data/test_featurized.csv\", index = False)\n",
    "    print(\"Featurization of train and test set Completed.........\")\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67feb9d4",
   "metadata": {
    "id": "67feb9d4"
   },
   "source": [
    "### Function to perform featurization on transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fab56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_featurization(df, column):\n",
    "    \"\"\"This function featurize the date column of a dataframe by engineering\n",
    "    new features such as year, month, day, hour etc. It takes the dataset as\n",
    "    dataframe and date column as string for input and returns the dataframe\n",
    "    with added features.\"\"\"\n",
    "    \n",
    "    df['year'] = df[column].dt.year\n",
    "    df['month'] = df[column].dt.month\n",
    "    df['dayofweek'] = df[column].dt.dayofweek\n",
    "    df['date'] = df[column].dt.day\n",
    "    df['hour'] = df[column].dt.hour\n",
    "    df['weekend'] = 0\n",
    "    df['weekend'][df['dayofweek'] >= 5] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "415ea07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_featurization(df, groupby, agg_dict, prefix = \"\"):\n",
    "    \"\"\"This function performs aggregation on a dataframe and returns the aggregate features\n",
    "    dataframe. It takes dataset as dataframe, groupby columns on which aggregate has to be performed\n",
    "    as list, aggregate functions to be performed on columns as dictionary and prefix to be added to\n",
    "    aggregated feature column name.\"\"\"\n",
    "    \n",
    "    agg_df = df.groupby(groupby).agg(agg_dict)\n",
    "    if prefix != \"\":\n",
    "        agg_df.columns = [prefix + '_' + '_'.join(col) for col in agg_df.columns.values]\n",
    "    else:\n",
    "        agg_df.columns = ['_'.join(col) for col in agg_df.columns.values]\n",
    "    agg_df.reset_index(inplace = True)\n",
    "    \n",
    "    return agg_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "553c53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_aggregate_featurization(df, columns, groupby, agg_dict, prefix = \"\"):\n",
    "    \"\"\"This function performs aggregation on a dataframe based on groupby and each categorical columns\n",
    "    and returns the aggregate features dataframe. It takes dataset as dataframe, groupby columns on\n",
    "    which aggregate has to be performed as list, categorical columns which have to be aggregated with\n",
    "    groupby columns as list and aggregate functions to be performed on columns as dictionary.\"\"\"\n",
    "    \n",
    "    df_features = pd.DataFrame(df['card_id'].unique(), columns = ['card_id'])\n",
    "    for col in columns:\n",
    "        agg_df = agg_featurization(df[df[col] == 1], groupby, agg_dict, prefix = prefix + \"_\" + col)\n",
    "        df_features = pd.merge(df_features, agg_df, on = 'card_id', how = 'left')\n",
    "        del agg_df\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f602f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_lag_aggregate_featurization(df, groupby, agg_dict, prefix = \"\"):\n",
    "    \"\"\"This function performs aggregation on a dataframe based on groupby and each value of month lag\n",
    "    columns and returns the aggregate features dataframe. It takes dataset as dataframe, groupby columns on\n",
    "    which aggregate has to be performed as list and aggregate functions to be performed on columns as\n",
    "    dictionary.\"\"\"\n",
    "    \n",
    "    df_features = pd.DataFrame(df['card_id'].unique(), columns = ['card_id'])\n",
    "    for value in df['month_lag'].unique():\n",
    "        agg_df = agg_featurization(df[df['month_lag'] == value], groupby, agg_dict,\n",
    "                                   prefix = prefix + '_month_lag_' + str(value))\n",
    "        df_features = pd.merge(df_features, agg_df, on = 'card_id', how = 'left')\n",
    "        del agg_df\n",
    "    \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bd2e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def successive_agg_featurization(df, groupby1, groupby2, columns, agg_dict, prefix = \"\"):\n",
    "    \"\"\"This function performs successive aggregation on a dataframe and returns the successive aggregate\n",
    "    features dataframe. It takes dataset as dataframe, groupby1 and groupby2 on which aggregate\n",
    "    has to be performed as strings, columns on which the aggregate function is to be performed and\n",
    "    aggregate functions to be performed on columns as dictionary.\"\"\"\n",
    "    \n",
    "    intermediate_agg_df = df.groupby([groupby1, groupby2])[columns].mean()\n",
    "    successive_agg_df = agg_featurization(intermediate_agg_df, groupby1, agg_dict, prefix = prefix + \"_\" + groupby2)\n",
    "    \n",
    "    return successive_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ed7dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFM_Score(x, col, rfm_quantiles):\n",
    "    \"\"\"Function to calculate Recency, Frequency and Monetary value score based on quantiles.\n",
    "    It takes respective value, column name and quantiles dataframe as input.\"\"\"\n",
    "\n",
    "    score_1 = 1\n",
    "    score_2 = rfm_quantiles.shape[0]\n",
    "    for i in range(rfm_quantiles.shape[0]):\n",
    "        if x <= rfm_quantiles[col].values[i]:\n",
    "            return score_2 if col is 'recency' else score_1\n",
    "        score_1 += 1\n",
    "        score_2 -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f4b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/rajeshcv/customer-loyalty-based-on-rfm-analysis/notebook\n",
    "def rfm_feature(df, quantiles):\n",
    "    \"\"\"This function performs the RFM featurization on dataset by generating the RFM score\n",
    "    and RFM index. It takes dataset as dataframe, and quantile values for scoring as list and\n",
    "    returns the RFM features as dataframe.\"\"\"\n",
    "    \n",
    "    agg_dict = {\n",
    "        'card_id'         : ['count'],\n",
    "        'purchase_date'   : ['max'],\n",
    "        'purchase_amount' : ['sum']\n",
    "    }\n",
    "    rfm_feature = agg_featurization(historical_transactions, groupby, agg_dict)\n",
    "    rfm_feature['recency'] = (datetime.date(2018, 3, 1) - rfm_feature['purchase_date_max'].dt.date).dt.days\n",
    "    rfm_feature.rename(columns = {'card_id_count' : 'frequency', 'purchase_amount_sum' : 'monetary_value'}, inplace = True)\n",
    "    rfm_feature = rfm_feature.drop(columns = ['purchase_date_max'])\n",
    "    \n",
    "    rfm_quantiles = rfm_feature.quantile(q = quantiles)\n",
    "    rfm_feature['R_score'] = rfm_feature['recency'].apply(RFM_Score, args = ('recency', rfm_quantiles))\n",
    "    rfm_feature['F_score'] = rfm_feature['frequency'].apply(RFM_Score, args = ('frequency', rfm_quantiles))\n",
    "    rfm_feature['M_score'] = rfm_feature['monetary_value'].apply(RFM_Score, args = ('monetary_value', rfm_quantiles))\n",
    "    rfm_feature['RFM_Score'] = rfm_feature['R_score'] + rfm_feature['F_score'] + rfm_feature['M_score']\n",
    "    rfm_feature['RFM_index'] = rfm_feature['R_score'].map(str) + rfm_feature['F_score'].map(str) + rfm_feature['M_score'].map(str)\n",
    "    rfm_feature['RFM_index'] = rfm_feature['RFM_index'].astype(int)\n",
    "    rfm_feature = rfm_feature.drop(columns = ['recency', 'frequency', 'monetary_value'])\n",
    "    \n",
    "    return rfm_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c61fe",
   "metadata": {
    "id": "012c61fe"
   },
   "source": [
    "We are engineering new columns from purchase date such as purchase year, month, weekday, date etc. These columns will be used to generate time related features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b5c3f9",
   "metadata": {},
   "source": [
    "Then we are engineering count of historical and new transactions features. These features will indicate the number of historical and new transactions done by each card id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6545e03",
   "metadata": {
    "id": "f6545e03"
   },
   "source": [
    "We are performing aggregation of card_id's to find different features for all historical and new transactions columns. We are using statistics such as sum, max, min, average, standard deviation etc for numerical columns and number of unique elements for categorical columns to generate features.\n",
    "1. authorized_flag features will indicate total authorized transactions and percentage of authorized transactions done by each card_id.\n",
    "2. category_1, category_2 and category_3 features will indicate total and percentage of transactions for that particular category value done by each card_id.\n",
    "3. merchant_id, merchant_category_id, subsector_id, city_id and state_id features will indicate number of unique merchants, merchant categories, subsectors, cities and states, each card_id did transaction at.\n",
    "4. month_lag features will indicate minimum and maximum transaction lag from reference date and and recency of transaction for each card_id.\n",
    "5. purchase date features will indicate the oldest and the newest date of transactions done by each card_id.\n",
    "6. year, month, dayofweek, date, hour features will indicate number of unique, mean, maximum and minimum of years, months, day of weeks, dates and hours, during which each card_id did transactions.\n",
    "7. weekend feature will indicate total and percentage of transactions done by each card_id on weekend.\n",
    "8. purchase_amount features will indicate total, average, maximum, minimum and variance of the amount spent by each card_id.\n",
    "9. installments features will indicate number of unique, total, average, maximum and minimum number of installments for transactions done by each card_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ebae1",
   "metadata": {
    "id": "447ebae1"
   },
   "source": [
    "We are also engineering some additional features from generated aggregate features.\n",
    "1. The difference in historical transaction count and historical authorized flag sum date will indicate the number of declined historical transactions for cardholders.\n",
    "2. The difference in maximum purchase date and minimum purchase date will indicate the duration in days when the transactions were done by each cardholders.\n",
    "3. The ratio of total purchase amount and duration of transactions will indicate the purchase amount spent per day by each cardholders.\n",
    "4. The difference in maximum purchase amount and minimum purchase amount will indicate the range of amount spent by each cardholders.\n",
    "5. The ratio of transaction count and duration of transactions will indicate the transactions done per day by each cardholders.\n",
    "6. The ratio of transaction count and unique number of merchant ids will indicate the transactions done per merchant by each cardholders.\n",
    "7. The ratio of transaction count and unique number of city ids will indicate the transactions done per city by each cardholders.\n",
    "8. The ratio of transaction count and unique number of state ids will indicate the transactions done per state by each cardholders.\n",
    "9. The ratio of transaction count and unique merchant category ids will indicate the transactions done per merchant category by each cardholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca86687",
   "metadata": {
    "id": "5ca86687"
   },
   "source": [
    "During EDA, we found that purchase amount had different distributions for different values of category 1, category 2 and category 3. We are performing aggregation of card id with different values of category 1, category 2 and category 3 columns and find features for purchase amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f9d7",
   "metadata": {
    "id": "06e5f9d7"
   },
   "source": [
    "We are also performing aggregation of card id for each month lag column values to find features for purchase amount. These features will indicate month wise features of purchase amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1499acb9",
   "metadata": {
    "id": "1499acb9"
   },
   "source": [
    "We are performing successive aggregation of card id and installments columns to find features for authorized flag and purchase amount. These features will indicate installment wise features of authorized flag and purchase amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f7f83",
   "metadata": {
    "id": "374f7f83"
   },
   "source": [
    "RFM is a market research tool for customer segmentation based on customer value to the firm. R stands for Recency, F for Frequency and M for Monetary value. Recency is the number of days since last purchase, Frequency is the total number of purchases and Monetary Value is the total money, the customer spent. An RFM analysis evaluates customers by scoring them in three categories: how recently they've made a purchase, how often they buy, and the size of their purchases.\n",
    "Based on target values, we will find quantiles which will be used to calculate scores for each card id. The card ids will be scored based on which quantile their recency, frequency and monetary values fall into. Also, recency will be scored opposite of frequency and monetary value i.e., smaller the recency value higher the score whereas larger the frequency and monetary values higher the score.\n",
    "The RFM score is the sum of the recency score, frequency score and monetary score while the RFM index is obtained by combining the recency score, frequency score and monetary score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ADAhYSShckRm",
   "metadata": {
    "id": "ADAhYSShckRm"
   },
   "outputs": [],
   "source": [
    "def feature_transactions(historical_transactions, new_transactions):\n",
    "    \"\"\"This function performs featurization on transactions dataset. It takes\n",
    "    transactions dataset as input and returns engineered features for each\n",
    "    card id as dataframe.\"\"\"\n",
    "    \n",
    "    print(\"Performing featurization on historical and new transactions.........\")\n",
    "    historical_transactions = date_featurization(historical_transactions, 'purchase_date')\n",
    "    new_transactions = date_featurization(new_transactions, 'purchase_date')\n",
    "    \n",
    "    hist_transactions_features = historical_transactions.groupby(['card_id']).size().reset_index()\n",
    "    hist_transactions_features.columns = ['card_id', 'hist_transc_count']\n",
    "    \n",
    "    new_transactions_features = new_transactions.groupby(['card_id']).size().reset_index()\n",
    "    new_transactions_features.columns = ['card_id', 'new_transc_count']\n",
    "\n",
    "    groupby = ['card_id']\n",
    "    agg_dict = {\n",
    "    'authorized_flag' : ['sum', 'mean'],\n",
    "    'category_1_0'    : ['sum', 'mean'],\n",
    "    'category_1_1'    : ['sum', 'mean'],\n",
    "    'category_2_1'    : ['sum', 'mean'],\n",
    "    'category_2_2'    : ['sum', 'mean'],\n",
    "    'category_2_3'    : ['sum', 'mean'],\n",
    "    'category_2_4'    : ['sum', 'mean'],\n",
    "    'category_2_5'    : ['sum', 'mean'],\n",
    "    'category_3_0'    : ['sum', 'mean'],\n",
    "    'category_3_1'    : ['sum', 'mean'],\n",
    "    'category_3_2'    : ['sum', 'mean'],\n",
    "       \n",
    "    'merchant_id'         : ['nunique'],\n",
    "    'merchant_category_id': ['nunique'],\n",
    "    'subsector_id'        : ['nunique'],\n",
    "    'city_id'             : ['nunique'],\n",
    "    'state_id'            : ['nunique'],\n",
    "\n",
    "    'month_lag'    : ['min', 'max', 'mean'],\n",
    "    'purchase_date': ['min', 'max'],\n",
    "    'year'         : ['nunique', 'mean', 'min', 'max'],\n",
    "    'month'        : ['nunique', 'mean', 'min', 'max'],\n",
    "    'dayofweek'    : ['nunique', 'mean', 'min', 'max'],\n",
    "    'date'         : ['nunique', 'mean', 'min', 'max'],\n",
    "    'hour'         : ['nunique', 'mean', 'min', 'max'],\n",
    "    'weekend'      : ['sum', 'mean'],\n",
    "\n",
    "    'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n",
    "    'installments'   : ['nunique', 'sum', 'mean', 'max', 'min']\n",
    "    }\n",
    "\n",
    "    hist_transactions_features = pd.merge(hist_transactions_features,\n",
    "                                      agg_featurization(historical_transactions, groupby, agg_dict, prefix = 'hist'),\n",
    "                                      on = 'card_id', how = 'left')\n",
    "    del agg_dict['authorized_flag']\n",
    "    new_transactions_features = pd.merge(new_transactions_features,\n",
    "                                     agg_featurization(new_transactions, groupby, agg_dict, prefix = 'new'),\n",
    "                                     on = 'card_id', how = 'left')\n",
    "    \n",
    "    hist_transactions_features['hist_denied_count'] = (hist_transactions_features['hist_transc_count'] -\n",
    "                                                   hist_transactions_features['hist_authorized_flag_sum'])\n",
    "    hist_transactions_features['hist_transaction_days'] = (hist_transactions_features['hist_purchase_date_max'] -\n",
    "                                                       hist_transactions_features['hist_purchase_date_min']).dt.days\n",
    "    hist_transactions_features['hist_purchase_amount_per_day'] = (hist_transactions_features['hist_purchase_amount_sum'] /\n",
    "                                                              (1 + hist_transactions_features['hist_transaction_days']))\n",
    "    hist_transactions_features['hist_purchase_amount_diff'] = (hist_transactions_features['hist_purchase_amount_max'] -\n",
    "                                                           hist_transactions_features['hist_purchase_amount_min'])\n",
    "    hist_transactions_features['hist_transactions_per_day'] = (hist_transactions_features['hist_transc_count'] /\n",
    "                                                           (1 + hist_transactions_features['hist_transaction_days']))\n",
    "    hist_transactions_features['hist_transactions_per_merchant_id'] = (hist_transactions_features['hist_transc_count'] /\n",
    "                                                                (1 + hist_transactions_features['hist_merchant_id_nunique']))\n",
    "    hist_transactions_features['hist_transactions_per_city_id'] = (hist_transactions_features['hist_transc_count'] /\n",
    "                                                               (1 + hist_transactions_features['hist_city_id_nunique']))\n",
    "    hist_transactions_features['hist_transactions_per_state_id'] = (hist_transactions_features['hist_transc_count'] /\n",
    "                                                                (1 + hist_transactions_features['hist_state_id_nunique']))\n",
    "    hist_transactions_features['hist_transactions_per_merchant_category_id'] =\\\n",
    "    (hist_transactions_features['hist_transc_count'] / (1 + hist_transactions_features['hist_merchant_category_id_nunique']))\n",
    "    hist_transactions_features = hist_transactions_features.drop(columns = ['hist_purchase_date_max',\n",
    "                                                                            'hist_purchase_date_min'])\n",
    "    hist_transactions_features = reduce_mem_usage(hist_transactions_features)\n",
    "    \n",
    "    new_transactions_features['new_transaction_days'] = (new_transactions_features['new_purchase_date_max'] -\n",
    "                                                     new_transactions_features['new_purchase_date_min']).dt.days\n",
    "    new_transactions_features['new_purchase_amount_per_day'] = (new_transactions_features['new_purchase_amount_sum'] /\n",
    "                                                            (1 + new_transactions_features['new_transaction_days']))\n",
    "    new_transactions_features['new_purchase_amount_diff'] = (new_transactions_features['new_purchase_amount_max'] -\n",
    "                                                         new_transactions_features['new_purchase_amount_min'])\n",
    "    new_transactions_features['new_transaction_per_day'] = (new_transactions_features['new_transc_count'] /\n",
    "                                                        (1 + new_transactions_features['new_transaction_days']))\n",
    "    new_transactions_features['new_transactions_per_merchant_id'] = (new_transactions_features['new_transc_count'] /\n",
    "                                                                 (1 + new_transactions_features['new_merchant_id_nunique']))\n",
    "    new_transactions_features['new_transactions_per_city_id'] = (new_transactions_features['new_transc_count'] /\n",
    "                                                             (1 + new_transactions_features['new_city_id_nunique']))\n",
    "    new_transactions_features['new_transactions_per_state_id'] = (new_transactions_features['new_transc_count'] /\n",
    "                                                              (1 + new_transactions_features['new_state_id_nunique']))\n",
    "    new_transactions_features['new_transactions_per_merchant_category_id'] = (new_transactions_features['new_transc_count'] /\n",
    "                                                        (1 + new_transactions_features['new_merchant_category_id_nunique']))\n",
    "    new_transactions_features = new_transactions_features.drop(columns = ['new_purchase_date_max', 'new_purchase_date_min'])\n",
    "    new_transactions_features = reduce_mem_usage(new_transactions_features)\n",
    "    \n",
    "    agg_dict = {\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'std']\n",
    "            }\n",
    "    category_col = ['category_1_0', 'category_1_1', 'category_2_1', 'category_2_2', 'category_2_3', 'category_2_4',\n",
    "                'category_2_5', 'category_3_0', 'category_3_1', 'category_3_2']\n",
    "    hist_category_features = category_aggregate_featurization(historical_transactions, category_col, groupby, agg_dict,\n",
    "                                                          prefix = 'hist')\n",
    "    hist_category_features = reduce_mem_usage(hist_category_features)\n",
    "    new_category_features = category_aggregate_featurization(new_transactions, category_col, groupby, agg_dict,\n",
    "                                                             prefix = 'new')\n",
    "    new_category_features = reduce_mem_usage(new_category_features)\n",
    "    \n",
    "    hist_month_lag_features = month_lag_aggregate_featurization(historical_transactions, groupby, agg_dict, prefix = 'hist')\n",
    "    hist_month_lag_features = reduce_mem_usage(hist_month_lag_features)\n",
    "    new_month_lag_features = month_lag_aggregate_featurization(new_transactions, groupby, agg_dict)\n",
    "    new_month_lag_features = reduce_mem_usage(new_month_lag_features)\n",
    "    \n",
    "    groupby1 = 'card_id'\n",
    "    groupby2 = 'installments'\n",
    "    columns = ['purchase_amount', 'authorized_flag']\n",
    "    agg_dict = {\n",
    "    'authorized_flag': ['sum', 'mean'],\n",
    "    'purchase_amount': ['sum', 'mean', 'min', 'max', 'std']\n",
    "    }\n",
    "    hist_installments_features = successive_agg_featurization(historical_transactions, groupby1, groupby2, columns, agg_dict,\n",
    "                                                          prefix = 'hist')\n",
    "    hist_installments_features = reduce_mem_usage(hist_installments_features)\n",
    "    new_installments_features = successive_agg_featurization(new_transactions, groupby1, groupby2, columns, agg_dict,\n",
    "                                                         prefix = 'new')\n",
    "    new_installments_features = reduce_mem_usage(new_installments_features)\n",
    "    \n",
    "    quantiles = [0.012, 0.02, 0.05, 0.2, 0.5, 0.8, 0.96, 0.992, 1.0]\n",
    "    hist_rfm_feature = rfm_feature(historical_transactions, quantiles)\n",
    "    hist_rfm_feature = reduce_mem_usage(hist_rfm_feature)\n",
    "    \n",
    "    all_transaction_features = pd.merge(hist_transactions_features, new_transactions_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, hist_category_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, new_category_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, hist_month_lag_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, new_month_lag_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, hist_installments_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, new_installments_features, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = pd.merge(all_transaction_features, hist_rfm_feature, on = 'card_id', how = 'left')\n",
    "    all_transaction_features = reduce_mem_usage(all_transaction_features)\n",
    "    all_transaction_features.to_csv('data/all_transaction_features.csv')\n",
    "    \n",
    "    del hist_transactions_features\n",
    "    del new_transactions_features\n",
    "    del hist_category_features\n",
    "    del new_category_features\n",
    "    del hist_month_lag_features\n",
    "    del new_month_lag_features\n",
    "    del hist_installments_features\n",
    "    del new_installments_features\n",
    "    del hist_rfm_feature\n",
    "    print(\"Featurization of historical and new transactions Completed.........\")\n",
    "\n",
    "    return all_transaction_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75f8a4b",
   "metadata": {},
   "source": [
    "## Final Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d3531cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(train, test, all_transaction_features):\n",
    "    \"\"\"This function prepares the final train data with all features and returns the\n",
    "    featurized train dataset. It takes train set and transaction features dataset as\n",
    "    dataframe.\"\"\"\n",
    "    \n",
    "    train = pd.merge(train, all_transaction_features, on = 'card_id', how = 'left')\n",
    "    train.fillna(value = 0, inplace = True)\n",
    "    test = pd.merge(test, all_transaction_features, on = 'card_id', how = 'left')\n",
    "    test.fillna(value = 0, inplace = True)\n",
    "    \n",
    "    train.to_csv('data/final_train.csv', index = False)\n",
    "    test.to_csv('data/final_test.csv', index = False)\n",
    "    \n",
    "    return train, test   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-H35ZY0csDh4",
   "metadata": {
    "id": "-H35ZY0csDh4"
   },
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BGvnufNUsNSm",
   "metadata": {
    "id": "BGvnufNUsNSm"
   },
   "source": [
    "### Function to build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "WEeDy6NGsYxT",
   "metadata": {
    "id": "WEeDy6NGsYxT"
   },
   "outputs": [],
   "source": [
    "def build_model(train):\n",
    "    \"\"\"This function build models from the train set and return the trained models.\n",
    "    It take featurized train dataframe as input.\"\"\"\n",
    "\n",
    "    print(\"Building the models.........\")\n",
    "    Y_train = train['target']\n",
    "    Outlier = train['outlier']\n",
    "    X_train = train.drop(columns = ['card_id', 'target', 'outlier'])\n",
    "    \n",
    "    parameters = {\n",
    "        'objective'        : 'reg:squarederror',\n",
    "        'learning_rate'    : 0.01,\n",
    "        'eval_metric'      : 'rmse',\n",
    "        'tree_method'      : 'gpu_hist',\n",
    "        'predictor'        : 'gpu_predictor',\n",
    "        'random_state'     : 9,\n",
    "        'verbosity'        : 0,\n",
    "        'max_depth'        : 7,\n",
    "        'subsample'        : 0.7145610313690366,\n",
    "        'colsample_bytree' : 0.364896100159906,\n",
    "        'min_split_loss'   : 2.2685374838074592,\n",
    "        'min_child_weight' : 16.579787389902428,\n",
    "        'reg_alpha'        : 9.874511648120071,\n",
    "        'reg_lambda'       : 3.474818860996104\n",
    "    }\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits = 4, shuffle = True, random_state = 9)\n",
    "    for fold, (train_idx, val_idx) in enumerate(folds.split(X_train, Outlier.values)):\n",
    "        train_data = xgb.DMatrix(X_train.iloc[train_idx], label = Y_train.iloc[train_idx])\n",
    "        val_data = xgb.DMatrix(X_train.iloc[val_idx], label = Y_train.iloc[val_idx])\n",
    "        reggressor_XGB = xgb.train(params = parameters, dtrain = train_data,\n",
    "                                   evals = [(train_data, 'train'), (val_data, 'eval')], num_boost_round = 10000,\n",
    "                                   early_stopping_rounds = 500, verbose_eval = False)\n",
    "        dump(reggressor_XGB, \"\".join(('data/Model', str(fold + 1), '.sav')))\n",
    "    print(\"Buiding of models Completed.........\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d1ae3f",
   "metadata": {},
   "source": [
    "### Function to predict target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4e825d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    '''This function predicts and returns the target value of test data.\n",
    "    It takes X_test as dataframe and regressor model for input.'''\n",
    "    \n",
    "    Y_test_pred = model.predict(xgb.DMatrix(X_test), iteration_range = (0, model.best_iteration))\n",
    "    \n",
    "    return Y_test_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f25e029",
   "metadata": {
    "id": "9f25e029"
   },
   "source": [
    "### Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9MdhzyzNo6uV",
   "metadata": {
    "id": "9MdhzyzNo6uV"
   },
   "outputs": [],
   "source": [
    "def function_1(card_id):\n",
    "    \"\"\"This function include entire pipeline, from data preprocessing to making final predictions.\n",
    "    It take in card_id from test as string for input and return loyalty score for the card_id.\"\"\"\n",
    "\n",
    "    if not os.path.exists(\"data/final_train.csv\") or not os.path.exists(\"data/final_test.csv\"):\n",
    "        \n",
    "        if not os.path.exists(\"data/train_featurized.csv\") or not os.path.exists(\"data/test_featurized.csv\") or\\\n",
    "        not os.path.exists(\"data/all_transaction_features.csv\"):\n",
    "            \n",
    "            if not os.path.exists(\"data/train_processed.csv\") or not os.path.exists(\"data/test_processed.csv\") or\\\n",
    "            not os.path.exists(\"data/historical_transactions_processed.csv\") or\\\n",
    "            not os.path.exists(\"data/new_transactions_processed.csv\"):\n",
    "                \n",
    "                train, test, historical_transactions, new_transactions = load_data()\n",
    "                train, test = process_train_test(train, test)\n",
    "                historical_transactions, new_transactions = process_transactions(historical_transactions, new_transactions)\n",
    "                train, test = feature_train_test(train, test)\n",
    "                all_transaction_features = feature_transactions(historical_transactions, new_transactions)\n",
    "                final_train, final_test = data_prepare(train, test, all_transaction_features)\n",
    "            \n",
    "            else:\n",
    "                train = pd.read_csv(\"data/train_processed.csv\")\n",
    "                test = pd.read_csv(\"data/test_processed.csv\")\n",
    "                train, test = feature_train_test(train, test)\n",
    "                historical_transactions = pd.read_csv(\"data/historical_transactions_processed.csv\")\n",
    "                new_transactions = pd.read_csv(\"data/new_transactions_processed.csv\")\n",
    "                all_transaction_features = feature_transactions(historical_transactions, new_transactions)\n",
    "                final_train, final_test = data_prepare(train, test, all_transaction_features)\n",
    "\n",
    "        else:\n",
    "            train = pd.read_csv('data/train_featurized.csv')\n",
    "            test = pd.read_csv('data/test_featurized.csv')\n",
    "            all_transaction_features = pd.read_csv('data/all_transaction_features.csv')\n",
    "            final_train, final_test = data_prepare(train, test, all_transaction_features)\n",
    "\n",
    "    else:\n",
    "        final_test = pd.read_csv('data/final_test.csv')\n",
    "        final_test = reduce_mem_usage(final_test)\n",
    "\n",
    "    if not os.path.exists(\"data/Model1.sav\") or not os.path.exists(\"data/Model2.sav\") or\\\n",
    "    not os.path.exists(\"data/Model3.sav\") or not os.path.exists(\"data/Model4.sav\"):\n",
    "        final_train = pd.read_csv('data/final_train.csv')\n",
    "        final_train = reduce_mem_usage(final_train)\n",
    "        build_model(final_train)\n",
    "\n",
    "    if str(card_id) in final_test['card_id'].astype('string').values:\n",
    "        X_test = final_test[final_test['card_id'] == str(card_id)].drop(columns = ['card_id'])\n",
    "        y = 0\n",
    "        for i in range(4):\n",
    "            model = load(\"\".join((\"data/Model\", str(i + 1), \".sav\")))\n",
    "            y += (predict(X_test, model) / 4)\n",
    "        return y\n",
    "    else:\n",
    "        print(\"Sorry, no data available for Card Id\", card_id)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GgLCU4NkwZaV",
   "metadata": {
    "id": "GgLCU4NkwZaV"
   },
   "source": [
    "### Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf110560",
   "metadata": {
    "id": "cf110560"
   },
   "outputs": [],
   "source": [
    "def function_2(card_id, target):\n",
    "    \"\"\"This function include entire pipeline, from data preprocessing to making final predictions.\n",
    "    It takes in card_id from test as string and loyalty score for input and returns the evaluation\n",
    "    mertric for the model.\"\"\"\n",
    "\n",
    "    Y_test_pred = function_1(card_id)\n",
    "    rmse = np.sqrt(mean_squared_error([target], [Y_test_pred]))\n",
    "\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ecc87dd",
   "metadata": {
    "id": "2ecc87dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Loyalty Score for Card ID C_ID_0ab67a22ab is -3.8906837105751038\n"
     ]
    }
   ],
   "source": [
    "card_id = \"C_ID_0ab67a22ab\"\n",
    "test_pred = function_1(card_id)\n",
    "print(\"The Loyalty Score for Card ID {} is {}\".format(card_id, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c29dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RSME Score for Model is 0.0\n"
     ]
    }
   ],
   "source": [
    "target = -3.8906837105751038\n",
    "rsme = function_2(card_id, target)\n",
    "print(\"The RSME Score for Model is\", rsme)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ede63162",
    "9386f0b6",
    "8c0ba355",
    "bbc86021",
    "pF0Xa3KZfJ1s",
    "iIqOAnR0fqRr",
    "dOMmHiRTs4Gc",
    "kFEXJx42piqT",
    "c3151ade",
    "otkdlfJIZZAQ",
    "ad8e32dd",
    "67feb9d4",
    "BGvnufNUsNSm"
   ],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
